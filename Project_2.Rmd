---
title: "Project_2"
author: "Maxim Serdakov"
date: "11/30/2020"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_section: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(ggplot2)
library(car)
library(gridExtra)
library(dplyr)
```

# Обязательная часть 

## Задание 1. Построение полной линейной модели. 

Сначала откроем данные и кратко посмотрим на них. 

```{r}
data <- Boston
str(data)
```

Заметим, что в данном случае все переменные численные кроме двух - это переменные chas, которая принимает только значения 0 и 1, а так же явно факторная переменная rad. В дальнейшем нам необходимо будет сравнить между собой коэффииценты между разными предикторами в модели. Но, так как стандартизация факторных переменных не имеет никакого смысла, проведем стандартизацию всех переменных, кроме этих двух. Заодно превратим их в непосредственно факторные.

```{r}
data_scale <- as.data.frame(sapply(data, scale))
data_scale$chas <- as.factor(data$chas)
data_scale$rad <- as.factor(data$rad)
```

Теперь, для стандартизированного датасета построим полную линейную модель без учета взаимодействия предикторов. 

```{r}
mod_scale <- lm(medv ~ ., data = data_scale)
summary(mod_scale)
```


## Задание 2. Диагностика полной линейной модели. 

Проведем диагностику данной модели на основании следующих условий:
а. Проверка линейности взаимосвязи
б. Проверка влиятельных наблюдений
в. Независимость наблюдений
г. Нормальность распределения и постоянство дисперсии

Произведем необходимый анализ с помощью функции fortify() из пакета ggplot. 

```{r}
mod_diag <- fortify(mod_scale)
head(mod_diag)
```

### Проверка независимости наблюдений. 

```{r}
durbinWatsonTest(mod_scale)
```

В данном случае значение критерия Дарбина-Уотсона меньше 2, то есть присутствует положительная автокорреляция. Значит, мы не можем сказать, что наблюдения в данном случае независимые.

### Проверка линейности взаимодействий и постоянства дисперсии. График остатков модели от предсказанных значений. 

```{r}
gg_resid <- ggplot(data = mod_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)+
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  ggtitle("График остатков для полной модели")

gg_resid  
```

Гетероскедактичности не обнаружено, то есть с дисперсиями все хорошо. Однако, есть большие проблемы с линейностью взимосвязи для корректности данной модели - довольно много точек на этом графике выбивается за пределы в +/- 2 стандартных отклонения. В начале и в конце есть большое количество положительных по этому фактору значений. 

### Проверка влиятельных наблюдений. График расстояний Кука.

```{r}
ggplot(, aes(x = 1:nrow(mod_diag), y = mod_diag$.cooksd)) + 
  geom_bar(stat = "identity")+ 
  geom_hline(yintercept = 4/nrow(data_scale), color = "red")+ 
  ggtitle("График расстояний Кука для полной модели")
```

Верхняя граница была посчитана по формуле из https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/RegressionOutliers. Видим, что немалое число наблюдений выбиваются за линию. Соответственно, в данных имеется немалое число выбросов. 

### Проверка на нормальность распеределения. График QQ-plot. 

```{r}
qqPlot(mod_diag$.fitted)
```

Определенно можно сказать, что большое число точек в правой части распределения имеют завышенные значения относительно тех, какими они должны были бы быть в нормальном распределении. Иными словами, распределение скорее всего вытянуто в левую сторону и вряд ли может считаться нормальным. 



Как итог, полная модель не является корректной, так как:

- наблюдаются проблемы с наличием влиятельных наблюдений;

- зависимости часто не являются линейными;

- распределение для значений всех предикторов не является нормальным;

- многие предикторы зависимы друг от друга, то есть имеются проблемы с мультиколлениарностью.

Соответственно, в дополнительной части работы мы будем пытаться оптимизировать модель для более точных предсказаний. 

## Задание 3. График предсказания стоимости от переменной с наибольшим коэффициентом.

Еще раз посмотрим на коэффициенты нашей модели. 

```{r}
summary(mod_scale)
```

Увидим, что наибольший из них по модулю - это коэффициент перед уровнем фактора rad, а точнее перед rad24. Это можно интепретировать как то, что наибольший вклад вносит эта факторная переменная - то есть самой большей стоимостью при прочих равных обладают дома, которые имеют 24-часовой доступ к круговым автомагистралям. Однако, не совсем ясно, как делать предсказание для факторной переменной, каким будет обладать смыслом такое предсказание зависимой переменной - ведь если значения для rad = 10 нет, то значит таких домов вероятнее всего не существует. А значит, не имеет практического смысла предсказывать medv для rad=10. Поэтому, сделаем так - построим график предсказания зависимой переменной на основе численной переменной с наибольшим по модулю коэффициентом - это переменная lstat. Но, чтобы учесть важность факторной переменной rad - сделаем это предсказание отдельно для каждого из уровней фактора. Тогда мы по умолчанию предполагаем важность rad - эта переменная учитывается первой, и уже разбив по ней мы предсказываем стоимость в зависимости от значений зависимой переменной со следующим наибольшим по модулю коэффициентом.

Сначала сделаем искуственный датасет для предсказывания результатов.

```{r}
MyData <- data.frame(
    rad = data_scale$rad,
    chas = data_scale$chas,
    crim = mean(data_scale$crim),
     zn = mean(data_scale$zn),
     nox = mean(data_scale$nox),
     rm = mean(data_scale$rm),
     age = mean(data_scale$age),
     dis = mean(data_scale$dis),
     tax = mean(data_scale$tax),
     ptratio = mean(data_scale$ptratio),
     black = mean(data_scale$black),
     lstat = seq(min(data_scale$lstat), max(data_scale$lstat),length.out = nrow(data_scale)),
     indus = mean(data_scale$indus))
Predictions <- predict(mod_scale, newdata = MyData,  interval = 'confidence')
data_predicted <- data.frame(MyData, Predictions)
```

Теперь построим график для результатов предсказаний. 

```{r}
graph_of_predict <- ggplot(data_predicted, aes(x = lstat, y = fit, color = rad)) +
  geom_ribbon(alpha = 0.2, aes(ymin = lwr, ymax = upr)) +
  geom_point() + 
  ggtitle("График предсказаний полной модели для предиктора ltstat по уровням rad")+
  facet_wrap(~rad)
graph_of_predict
```

# Дополнительная часть 

Попробуем теперь разобраться, как мы можем улучшить нашу модель. В первую очередь, будем избавляться от мультиколлениарности. Если посмотреть на структуру датасета, то можно обнаружить, несмотря на сложность такого восприятия из-за большого числа предикторов, очень разнообразные отношения между предикторами, претендующие на зависимость. 

```{r}
plot(data)
```

Так как перед нами стоит задача сообщить заказчику на каких моментах стоит сосредоточить свое внимание, чтобы максимизировать цену за проданный дом, можем продолжить работу со стандартизированной моделью. Дело в том, что чтобы обладать информацией о таких аспектах, необходимо будет сранивать между собой влияния различных предикторов. Для того, чтобы описать заказчику идеальный самый прибыльный район, собственно и необходима наиболее корректная - то есть максимально правильно предсказывающая значение цены дома от значимых предикторов модель, в которой нужно будет выделить самые влиятельные аспекты. Для того, чтобы в итоге построить предсказание, нам нужно будет выделить собственно предиктор с самым большим коэффициентом. Плюс, стандартизированные данные дают более удобные для восприятия численные значения, что может отобразиться на виде графиков (в лучшую сторону). 

## Устранение мультиколлениарности 

Итак, займемся улучшением полной модели, для начала проверив модель на мультиколлениарность предикторов с использованием VIF. Будем исключать из модели те предикторы, значение VIF для которых больше 2. 

```{r}
mod_full <- lm(medv ~ ., data = data_scale)
vif(mod_full)
```

Видим, что из-за существования факторной переменной rad с таким большим количеством градаций фактора, стандартная функция считает вместо показателя VIF показатель GVIF - generalized VIF, являющийся рекоммендованным для работы с показателями с более чем двумя уровнями. Однако, судя по всему, показатель тем не менее обрабатывается не совсем корректно и принимает значение больше 10. В любом случае, GVIF равный 18 для предиктора rad является значимым поводом удалить этот предиктор из дальнейшей модели. Тем более, что если бы мы не принимали rad в качестве фактора, а работали с ним как с численной переменной, то значение обычного VIF для него тем не менее превышало бы 2. 

```{r}
data_scale$chas <- as.numeric(data_scale$chas)
data_scale$rad <- as.numeric(data_scale$rad)
mod_full_without_factors <- lm(medv ~ ., data = data_scale)
vif(mod_full_without_factors)
data$chas <- as.factor(data$chas)
```

Значит, в любом случае удаление rad было бы корректно - поэтому удалим его из нашей модели первым чтобы в дальнейшем работать с более понятными показателями VIF. 

```{r}
mod_2 <- update(mod_scale, .~. - rad)
vif(mod_2)
```

Теперь переменную nox:

```{r}
mod_3 <- update(mod_2, .~. - nox)
vif(mod_3)
```

Далее без отдельных комментариев будем удалять предикторы из модели, пока не останутся только предикторы с показателями не превышающими 2.

```{r}
mod_4 <- update(mod_3, .~. - dis)
vif(mod_4)
```

```{r}
mod_5 <- update(mod_4, .~. - indus)
vif(mod_5)
```

```{r}
mod_6 <- update(mod_5, .~. - lstat)
vif(mod_6)
```

```{r}
mod_7 <- update(mod_6, .~. - tax)
vif(mod_7)
```

Можно теперь оценить, какой вид имеет график для данных, из которых исключены предикторы, дающие мультиколлениарность. 

```{r}
data_without_multi <- data_scale[,-c(3, 5, 8, 9, 10, 13)]
plot(data_without_multi)
```

### Диагностика 

Собственно, можно оценить, стала ли модель лучше по интересующим нас параметрам. Для этого проведем частичный аналогичный первой части анализ. 

```{r}
summary(mod_7)
```

Сразу можно обратить внимание, что у модели только ухудшился показатель улучшенного R-квадрат, так что с ней точно стоит пробовать проводить дальнейшие улучшения. 

Так же можно построить график остатков:

```{r}
mod_7_diag <- data.frame(fortify(mod_7), data_scale[,c(3, 5, 8, 9, 10, 13)])

gg_resid <- ggplot(data = mod_7_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)+
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  ggtitle("График остатков модели с исключенной мультиколлениарностью")

gg_resid  
```

И график расстояний Кука:

```{r}
ggplot(, aes(x = 1:nrow(mod_7_diag), y = mod_7_diag$.cooksd)) + 
  geom_bar(stat = "identity")+ 
  geom_hline(yintercept = 4/nrow(data_without_multi), color = "red")+ 
  ggtitle("График расстояний Кука модели с исключенной мультиколлениарностью")
```

А так же график qq-plot.

```{r}
qqPlot(mod_7_diag$.fitted)
```

Несмотря на благие намерения исключения мультиколлениарности, модель стала хуже по параметру улучшенного R-квадрата, а графики остатков и расстояний кука значимо (можно увидеть, что число точек, выбивающихся за 2 стандартных отклонения немного уменьшилось, а пиков на графике расстояний Кука стало немного меньше) не изменились. График qq-plot же как будто стал еще хуже, потому что еще большее число точек и в конце, и в начале стали выбиваться за значения, соответствовавшие бы нормальному распределению. Будем дальше улучшать модель путем подбора исключительно значимых параметров. 

## Пошаговый отбор предикторов по значимости 

Будем последовательно удалять предикторы, если при сравнивания модели без предиктора и более полной модели на основании частного F-теста выясняется, что предиктор незначимо объясняет изменчивость зависимой переменной, то такой предиктор мы будем удалять. 

```{r}
drop1(mod_7, test = "F")
```

```{r}
mod_8 <- update(mod_7, .~. - zn)
drop1(mod_8, test = "F")
```

Увидим, что нам пришлось исключить только 1 предиктор zn. Просмотрим, объясняет ли теперь модель изменчивость зависимой переменной лучшим образом. 

### Диагностика 

```{r}
summary(mod_8)
```

Улучшенный R-квадрат стал чуть лучше, но все еще хуже, чем у полной модели, так что наши преобразования однозначно не закончены. 

```{r}

mod_8_diag <- data.frame(fortify(mod_8), data_scale[,c(2, 3, 5, 8, 9, 10, 13)])

gg_resid <- ggplot(data = mod_8_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)+
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  ggtitle("График остатков оптимизированной модели")

gg_resid 
```

```{r}
ggplot(, aes(x = 1:nrow(mod_8_diag), y = mod_8_diag$.cooksd)) + 
  geom_bar(stat = "identity")+ 
  geom_hline(yintercept = 4/nrow(data_scale), color = "red")+ 
  ggtitle("График расстояний Кука оптимизированной модели")
```

```{r}
qqPlot(mod_8_diag$.fitted)
```

На вид, устранение предикторов, дававших мультиколлениарность, а так же незначимых предикторов, не исправило ни один из недостатков нашей модели, и в принципе мы добились только ухудшения в нашей цели объяснения изменчивости зависимой переменной. То есть наша модель сейчас:

- все еще обладает некоторой степенью нелинейности; 
- распределения значений предикторов не являются нормальными; 
- имеются влиятельные наблюдения; 

## Коррекция оптимальной модели 

Наша проблема может быть связана с тем, что мы исключили слишком много предикторов на стадии исключения предикторов, дающих мультиколлениарность, то есть сильно коррелировавших с другими предикторами. В ходе этого процесса мы могли потерять какие-то важные зависимости, дающие большой вклад в изменчивость независимой переменной. Для определения этого построим диагностические графики с учетом тех предикторов, которые мы выкинули на стадии отбора по VIF:

```{r}
res_1 <- gg_resid + aes(x = data_scale$rad) + ggtitle("Остатки с учетом rad")
res_2 <- gg_resid + aes(x = data_scale$nox) + ggtitle("Остатки с учетом nox")
res_3 <- gg_resid + aes(x = data_scale$dis) + ggtitle("Остатки с учетом dis")
res_4 <- gg_resid + aes(x = data_scale$indus) + ggtitle("Остатки с учетом indus")
res_5 <- gg_resid + aes(x = data_scale$lstat) + ggtitle("Остатки с учетом lstat")
res_6 <- gg_resid + aes(x = data_scale$tax) + ggtitle("Остатки с учетом tax")
grid.arrange(res_1, res_2, res_3, res_4,res_5, res_6,nrow = 2)
```

Увидим, что мы явно могли потерять зависимость предсказываемой переменной от параметров dis и lstat. Попробуем вернуть эти предикторы в оптимальную модель, несмотря на их вклад в мультиколлениарность, так как из-за этого скорее всего и падает улучшенный R-квадрат. 

```{r}
mod_8_1 <- update(mod_8, .~. + dis + lstat)
summary(mod_8_1)
```

Действительно, нам удалось значительно поднять параметр улучшенного R-квадрат путем возвращения предикторов, дающих мультиколлениарность. Посмотрим, не стали ли какие-то предикторы теперь так же незначимыми и уберем их в случае обнаружения. 

```{r}
drop1(mod_8_1, test = "F")
```

Увидим, что ни один из предикторов не является незначимым, то есть p-value ни у одного не превышает 0.05. Однако, у двух предикторов - crim и age - уровни значимости близки к 0.05. Попробуем посмотреть на модели, где уберем эти предикторы - не станут ли они лучше? 
```{r}
mod_8_1.1<-update(mod_8_1, .~. - crim)
summary(mod_8_1.1)
```

```{r}
mod_8_1.2<-update(mod_8_1, .~. - age)
summary(mod_8_1.2)
```

```{r}
mod_8_1.3<-update(mod_8_1, .~. - age - crim)
summary(mod_8_1.3)
```

В каждой из таких моделей параметр R-квадрат падает, хоть и не сильно, однако видимых причин исключить предикторы age и crim не обнаружилось - формально они проходят установленное значение p-value, а модель как минимум не улучшается. Посмотрим, однако, ради интереса, на графики остатков и расстояний Кука для данных моделей. 

```{r}
mod_8_1.1_diag <- data.frame(fortify(mod_8_1.1), data_scale[,c(1, 2, 3, 5, 9, 10)])
mod_8_1.2_diag <- data.frame(fortify(mod_8_1.2), data_scale[,c(2, 3, 5, 7, 9, 10)])
mod_8_1.3_diag <- data.frame(fortify(mod_8_1.3), data_scale[,c(1, 2, 3, 5, 7, 9, 10)])

gg_resid_8_1.1 <- ggplot(data = mod_8_1.1_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)+
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  ggtitle("Остатки 8_1.1")

gg_resid_8_1.2 <- ggplot(data = mod_8_1.2_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)+
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  ggtitle("Остатки 8_1.2")

gg_resid_8_1.3 <- ggplot(data = mod_8_1.3_diag, aes(x = .fitted, y = .stdresid)) + 
  geom_point() + 
  geom_hline(yintercept = 0)+
  geom_smooth(method = "lm") +
  geom_hline(yintercept = 2, color = "red") +
  geom_hline(yintercept = -2, color = "red") +
  ggtitle("Остатки 8_1.3")

cook_8_1.1 <- ggplot(, aes(x = 1:nrow(mod_8_1.1_diag), y = mod_8_1.1_diag$.cooksd)) + 
  geom_bar(stat = "identity")+ 
  geom_hline(yintercept = 4/nrow(data_scale), color = "red")+ 
  ggtitle("Расстояния Кука 1.1")

cook_8_1.2 <- ggplot(, aes(x = 1:nrow(mod_8_1.2_diag), y = mod_8_1.2_diag$.cooksd)) + 
  geom_bar(stat = "identity")+ 
  geom_hline(yintercept = 4/nrow(data_scale), color = "red")+ 
  ggtitle("Расстояния Кука 1.2")

cook_8_1.3 <- ggplot(, aes(x = 1:nrow(mod_8_1.3_diag), y = mod_8_1.3_diag$.cooksd)) + 
  geom_bar(stat = "identity")+ 
  geom_hline(yintercept = 4/nrow(data_scale), color = "red")+ 
  ggtitle("Расстояния Кука 1.3")

grid.arrange(gg_resid_8_1.1, gg_resid_8_1.2, gg_resid_8_1.3, cook_8_1.1, cook_8_1.2, cook_8_1.3,nrow = 2)

```

У модели, в которой не присутствует предиктор crim, становится значительно лучше график расстояний Кука, выбросы становятся менее значительными. Несмотря на уменьшение параметра улучшенного R-квадрат, тем не менее примем за финальную модель именно вариант 8_1.1, без предиктора crim, чтобы иметь возможность улучшить модель хотя бы по некоторым диагностическим показателям. 

Для окончательной диагностики построим так же график qq-plot для нашей принятой за финальную модели:

```{r}
qqPlot(mod_8_1.1_diag$.fitted)
```

Нормальность у модели не стала значительно лучше, чем у полной модели, однако это распределение однозначно выглядит лучше, чем у модели, бывшей финальной до данного этапа - до возвращения предикторов dis и lstat и исключения crim. Проверим так же модель на наличие автокорреляций. 

```{r}
durbinWatsonTest(mod_8_1.1)
```

Значение критерия Дарбина-Уотсона все еще довольно далеко от 2, но это объясняется тем, что мы возвращали предикторы, которые исключали по параметру VIF. 

Итого, наша финальная модель, после всех доступных нам улучшений:

- проще, чем изначальная модель, так как зависит от меньшего числа переменных;
- обладает более выраженной нормальностью относительно полной модели;
- не удалось добиться линейности данных;
- обладает лучшей ситуацией с влиятельными наблюдениями; 
- параметр улучшенный R-квадрат немного уменьшился, но значимо не изменился; 
- нам пришлось оставить предикторы, дающие мультиколлениарность, чтобы не убирать важные зависимости для зависимой переменной. 

## Итоговая модель и ее предсказания 

Посмотрим на нашу итоговую модель и сделаем по ней выводы. Итак, вот таким образом выглядит наш ответ для заказчика. 

```{r}
summary(mod_8_1.1)
```

Получившееся в итоге уравнение для зависимой переменной имеет вид:

$$medv = { 0.30919*chas + 0.34627*rm - 0.07965*age - 0.21019*plratio + 0.11647*black -0.17331*dis - 0.43778*lstat - 0.33057}$$
Осуществим, собственно, предсказание по "улучшенной" линейной модели. Аналогично первой части, будем ориентироваться на предиктор с самым большим значением коэффициента, то есть опять же lstat. Для этого сначала создадим тестовый датасет:

```{r}
MyData_for_8_1.1 <- data.frame(
     chas = data_scale$chas,
     rm = mean(data_scale$rm),
     age = mean(data_scale$age),
     dis = mean(data_scale$dis),
     ptratio = mean(data_scale$ptratio),
     black = mean(data_scale$black),
     lstat = seq(min(data_scale$lstat), max(data_scale$lstat),length.out = nrow(data_scale)))
Predictions_for_8_1.1 <- predict(mod_8_1.1, newdata = MyData_for_8_1.1,  interval = 'confidence')
data_predicted_adj <- data.frame(MyData_for_8_1.1, Predictions_for_8_1.1)
```

И затем построим график:

```{r}
Pl_predict_adj <- ggplot(data_predicted_adj, aes(x = lstat, y = fit)) +
  geom_ribbon(alpha = 0.2, aes(ymin = lwr, ymax = upr)) +
  geom_line() + 
  ggtitle("Предсказания на улучшенной модели")
Pl_predict_adj
```

## Выводы по данным и модели

Господин заказчик!
К сожалению, качество поступивших на обработку данных не позволило построить достаточно качественную линейную модель для предсказания цены дома. Тем не менее, многодневная работа нашего агенства позволила построить линейную модель, обладающую некоторыми преимуществами по сравнению с полной моделью, так что мы можем дать Вам совет по поводу того, на каких именно аспектах стоит сосредоточиться, чтобы извлекать максимальную выгоду из операций с недвижимостью в славном Бостоне в наши тяжелые 70е года. Эта модель обладает относительно нормальным распределением для остатков, исключает большое число влиятельных наблюдений, а так же обладает постоянством дисперсии. Все эти успехи позволяют нам быть уверенными в том, что Вы не зря обратились именно к нам, а так же что наша работа будет соотвественно высоко оценена и оплачена Вами, а в дальнейшем мы сможем продолжить выгодное сотрудничество. 

С Вашего позволения, креативный отдел нащего агенства предоставит Вам запрашиваемые данные в виде стихотворения:

```
Добрый день. Господин клиент, 
Чтобы денег побольше Вам обрести
Наше агенство дает инструмент
Для успеха на рынке недвижимости.

Во избежании всяких там казусов 
И выгод как можно больше дальнейших
Ищите район, где низкого статуса
Процент населения как можно меньше.
```

Так же, помимо параметра lstat, обозначающего процент населения с низким статусом, важное положительное влияние на цену дома оказывает число комнат в нем (rm) и находится ли участок рядом с рекой (факторная переменная chas). Снижают же цену такие параметры как малое количество учителей, приходящихся на одного человека в районе (plratio), большая доля занятых владельцами домов, построенных до 1940 года (age), слишком большая удаленность от 5 Бостонских центров занятности (dis), а так же высокий процент афроамериканского населения в районе (black). Исходя из комбинаций этих параметров и стоит в первую очередь выбирать дом. 

